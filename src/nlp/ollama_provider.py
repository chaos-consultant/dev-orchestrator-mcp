"""
Ollama NLP provider for local model inference.
"""

import aiohttp
import json
from typing import Dict, Any
from .base import NLPProvider, NLPContext, NLPResult


# Available Ollama models optimized for code/commands
OLLAMA_MODELS = {
    'codellama:7b-instruct': {
        'name': 'CodeLlama 7B Instruct',
        'description': 'Fast, good for simple commands',
        'size': '3.8GB',
        'recommended': True,
    },
    'codellama:13b-instruct': {
        'name': 'CodeLlama 13B Instruct',
        'description': 'More accurate, moderate speed',
        'size': '7.4GB',
        'recommended': False,
    },
    'phind-codellama:34b-v2': {
        'name': 'Phind CodeLlama 34B',
        'description': 'Most accurate, slower',
        'size': '19GB',
        'recommended': False,
    },
    'wizardcoder:7b': {
        'name': 'WizardCoder 7B',
        'description': 'Specialized for code generation',
        'size': '3.8GB',
        'recommended': False,
    },
    'wizardcoder:13b': {
        'name': 'WizardCoder 13B',
        'description': 'Balanced performance and accuracy',
        'size': '7.4GB',
        'recommended': False,
    },
    'deepseek-coder:6.7b-instruct': {
        'name': 'DeepSeek Coder 6.7B',
        'description': 'Excellent for developer tasks',
        'size': '3.8GB',
        'recommended': True,
    },
}


class OllamaProvider(NLPProvider):
    """Provider that uses Ollama for local LLM inference."""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.base_url = config.get('ollama_url', 'http://localhost:11434')
        self.model = config.get('model', 'codellama:7b-instruct')
        self.max_tokens = config.get('max_tokens', 512)
        self.temperature = config.get('temperature', 0.1)

    def _build_prompt(self, user_input: str, context: NLPContext) -> str:
        """Build an enhanced prompt for command translation."""
        prompt = f"""You are a shell command expert. Translate natural language requests into shell commands.

Context:
- Current directory: {context.cwd}
- Operating System: {context.os_type}
- Shell: {context.shell_type}
"""

        if context.project_type:
            prompt += f"- Project type: {context.project_type}\n"

        if context.recent_commands:
            recent = context.recent_commands[-5:]  # Last 5 commands
            prompt += f"- Recent commands: {', '.join(recent)}\n"

        prompt += """
Rules:
1. Output ONLY the shell command, no explanations or markdown
2. Use common flags and best practices
3. For destructive commands, add safety flags (e.g., -i for interactive)
4. Use modern command alternatives when available (e.g., ripgrep over grep)
5. If the request is ambiguous, choose the most common interpretation

Request: {user_input}

Command:"""

        return prompt

    async def translate(
        self,
        user_input: str,
        context: NLPContext
    ) -> NLPResult:
        """Translate using Ollama API."""
        prompt = self._build_prompt(user_input, context)

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": self.temperature,
                            "num_predict": self.max_tokens,
                        }
                    },
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status != 200:
                        raise Exception(f"Ollama API error: {response.status}")

                    data = await response.json()
                    command = data.get('response', '').strip()

                    # Clean up the response
                    command = self._clean_command(command)

                    return NLPResult(
                        command=command,
                        confidence=0.8,  # Ollama doesn't provide confidence scores
                        source='ollama',
                        explanation=f"Generated by {self.model}"
                    )

        except aiohttp.ClientError as e:
            raise Exception(f"Failed to connect to Ollama: {str(e)}")
        except Exception as e:
            raise Exception(f"Ollama translation failed: {str(e)}")

    def _clean_command(self, command: str) -> str:
        """Clean up the generated command."""
        # Remove markdown code blocks
        if command.startswith('```'):
            lines = command.split('\n')
            command = '\n'.join(lines[1:-1]) if len(lines) > 2 else command

        # Remove common prefixes
        prefixes = ['$ ', '> ', '# ', 'shell> ', 'bash> ']
        for prefix in prefixes:
            if command.startswith(prefix):
                command = command[len(prefix):]

        # Remove trailing explanations
        if '\n' in command:
            command = command.split('\n')[0]

        return command.strip()

    async def test_connection(self) -> bool:
        """Test connection to Ollama server."""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{self.base_url}/api/tags",
                    timeout=aiohttp.ClientTimeout(total=5)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        models = [m['name'] for m in data.get('models', [])]
                        # Check if our configured model is available
                        return any(self.model in m for m in models)
                    return False
        except Exception:
            return False

    @staticmethod
    def get_available_models() -> Dict[str, Dict[str, Any]]:
        """Get list of recommended Ollama models."""
        return OLLAMA_MODELS

    async def list_installed_models(self) -> list[str]:
        """List models installed in Ollama."""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{self.base_url}/api/tags",
                    timeout=aiohttp.ClientTimeout(total=5)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return [m['name'] for m in data.get('models', [])]
                    return []
        except Exception:
            return []
